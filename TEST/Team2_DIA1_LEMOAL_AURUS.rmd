---
title: "Team2_DIA1_LEMOAL_AURUS"
date: '2022-12-09'
author: Aurus Aurelien LE MOAL Steven
output: 
 rmdformats::readthedown:
    highlight: kate
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Music Dataset

Student : Aurelien AURUS, Stevn LE MOAL

#### Introduction

A dataset of 1613 songs with attributes from Spotify for some users

Every song is labeled in the column save:

"1" means the user likes the song .
"0" means he doesn't like it.
There are 16 columns in the dataset. 2 of them describe the song : song 's name and artist. Another, the save column, is the target variable

The other 13 columns are the audio features of a song.

More info : https://developer.spotify.com/documentation/web-api/reference/#/operations/get-audio-features

#### Problematique

We want to predict if a user, will like or not a music.
The evaluation metric for this competition is the accuracy (proportion of the correct predictions)


#### Data Processing

##### Library and Data Overview

```{r message=FALSE}
library(magrittr)
library("dplyr")
library("ggplot2")
library(tidyr)
library(corrplot)
library(rpart)
#library(MASS)
library(caTools)
library(rpart.plot)
library(randomForest)
library(gbm)
library(xgboost)
library(caret)
library(ipred)
library(ROCR)
library(FactoMineR)
library(factoextra)
library(mlbench)
library(e1071)
library(MLmetrics)

set.seed(2)
```


```{r}
data = read.csv("C:/Users/Papa/Documents/R_eval/train.csv")
head(data)
```
acousticness : A confidence measure from 0 to 1 of whether the song is acoustic. 1.0 represents high confidence the song is acoustic

danceability : measure from 0.0 to 1.0 describes how suitable a song is for dancing. A value of 0 is least danceable and 1 is most danceable

duration_ms : The duration of the song in milliseconds

energy : Energy is a measure from 0 to 1 and represents a perceptual measure of intensity and activity

instrumentalness : It predicts whether a song contains no vocals. The closer the instrumentals value is to 1.0

key : The key the song is in. Integers map to pitches using standard Pitch Class notation. Eg. 0= C,1=C# etc..

liveness : Detects the presence of an audience in the recording. A value above 0.8 provides a strong likelihood that the song is live.

loudness : The overall loudness of a song in decibels(dB)

mode : It indicates the modality(major or minor) of the song

speechiness : Speechiness detects the presence of spoken words in a song . The more exclusively speech-like the recording, the closer to 1.0 the attribute value.

tempo : The overall estimated tempo of a song in beats per minute (BPM)

time_signature : An estimated overall time signature of a song .

valence : A measure from 0.0 to 1.0 describing the musical positiveness conveyed by a song .

```{r}
summary(data)
```

Number of null value by variable

```{r}
sapply(data, function(x) sum(is.na(x)))
```

Number of unique value by variable

```{r}
sapply(data, function(x) length(unique(x)))
```

Distribution of each predictor (depending on the range of the value)

```{r}
data %>% 
  select_if(is.numeric) %>% 
  select(-c(duration_ms, tempo, loudness,key,time_signature)) %>% 
  boxplot(main = 'Distribution of Each Predictor', xlab = 'Predictor', ylab = 'Values')
```

```{r}
data %>% 
   select_if(is.numeric) %>% 
   select(c(time_signature, loudness,key)) %>% 
   boxplot(main = 'Distribution of Each Predictor', xlab = 'Predictor', ylab = 'Values')
```

```{r}
data %>% 
   select_if(is.numeric) %>% 
   select(c(tempo)) %>% 
   boxplot(main = 'Distribution of Tempo', xlab = 'Predictor', ylab = 'Values')
```

It's interesting to see the value of the variable to see if they have a lot of high/low value.

We see if the features have outliers and we use the t-test to know if we can remove them :

```{r message=FALSE}
out_acc = boxplot.stats(data$acousticness)$out
out_dan = boxplot.stats(data$danceability)$out
out_dur = boxplot.stats(data$duration_ms)$out
out_ene = boxplot.stats(data$energy)$out
out_ins = boxplot.stats(data$instrumentalness)$out
out_liv = boxplot.stats(data$liveness)$out
out_lou = boxplot.stats(data$loudness)$out
out_spe = boxplot.stats(data$speechiness)$out
out_tem = boxplot.stats(data$tempo)$out

data.outliers = data %>% filter(acousticness >= out_acc | 
                           danceability >= out_dan | 
                           duration_ms >= out_dur | 
                           energy >= out_ene | 
                           instrumentalness >= out_ins |
                           liveness >= out_liv |
                           loudness >= out_lou |
                           speechiness >= out_spe|
                           tempo >= out_tem
                          )
                          
data.clean <- data %>% filter(!song_title %in% data.outliers$song_title)
```

```{r}
t.test(data$save, data.outliers$save)
```
We see that p > 0,05 so it is not safe to assume that the strengths of the normal and outlier groups differ.

```{r}
#data.clean %>% 
#    ggplot(aes(x = duration_ms, y = save)) +
#    geom_point() + 
#    geom_point(data = data.outliers, aes(x = duration_ms, y = save), col = 'red') + 
#    labs(
#        title = 'Distribution of Save : Original vs outlier (red)',
#        x = 'duration',
#        y = 'Save')
```


# Distribution of each feature (density graph)

```{r message=FALSE}
data %>% 
    select_if(is.numeric) %>% 
    pivot_longer(cols = -save, names_to = 'predictor') %>% 
    ggplot(aes(x = value)) +
    geom_density() +
    facet_wrap(~predictor, scales = 'free_x')
```


```{r}
data %>% 
    select_if(is.numeric) %>% 
    pivot_longer(cols = -save, names_to = 'predictor') %>% 
    ggplot(aes(x = value, y = save)) +
    geom_point() +
    geom_smooth(method = 'loess', formula = 'y ~ x') +
    facet_wrap(~predictor, scales = 'free_x')
```
Some variable like key and mode are really uniform. The variable' graphs will help use choice our predictors (like energy..) since we want 'something' linear.

```{r}
#ggplot(data = data, aes(x = (save))) +
#  geom_density(fill = "red", color = "black") +
#  labs( title = "Like Distribution", x = "Variable")
```


One of the approach that could be use is scaling (but for the model, we didnt use the scale ones)

```{r}
data_scale = data %>% select_if(is.numeric) 
data_scale[,-c(14)] = scale(data_scale[,-c(14)], center = TRUE, scale = TRUE)
data_scale %>% 
   boxplot(main = 'Distribution of Each Predictor', xlab = 'Predictor', ylab = 'Values')
```
Distribution of variables

```{r}
data_scale %>% 
    select_if(is.numeric) %>% 
    pivot_longer(cols = -save, names_to = 'predictor') %>% 
    ggplot() +
    geom_histogram(aes(x = value), bins = 15, color = 'black', fill = 'white') +
    facet_wrap(~predictor, scales = 'free_x')
```
Some variable are uniform, but some other could dont have a good distribution curve (some transformation on the feature like log or log(1+x) could be apply)

We can look at the correlation of the predictors with 'save':

```{r}
data_scale.cor = round(cor(data_scale), 2)
cor_ = data.frame(data_scale.cor[,c("save")])
cor_$Category = row.names(cor_)
cor_$Values = cor_$data_scale.cor...c..save...


cor_ %>% 
  ggplot(aes(reorder(Category, Values), Values)) + 
    geom_col() + 
    coord_flip() + 
    labs(x = "Correlation") + 
    theme_classic()
```

# PCA visualization

We use PCA evaluation to select the best predictors (less redondant information)

```{r}
tmp = data[,unlist(lapply(data, is.numeric))] %>% select(-c(save))
res.pca <-PCA(tmp, scale.unit = TRUE, graph = TRUE)
```

```{r}
summary(res.pca)
```

```{r}

get_eigenvalue(res.pca)

```

```{r}
fviz_eig(res.pca,addlabels = TRUE)

```

```{r}
#Plot the correlation circle using the fviz_pca_var function.
fviz_pca_var(res.pca, col.var = "cos2",
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"))

```

```{r}
var <- get_pca_var(res.pca)
var

library("corrplot")
corrplot(var$cos2)
```

```{r}
fviz_contrib(res.pca, choice = "var", axes = 1:4)

```
we will take the first 7th features.

We can select variables based on the PCA evaluation, or to be more precise we can elimate variables because they don't give additionnal informations. We can for example take the 7 best features ( > 70%).


# ML Model (for kaggle models we used all the features, in this session we look only to the selected features)

```{r message = FALSE}
df = data
df$save=as.factor(df$save)

sample = sample(1:nrow(df), nrow(df)*0.8) 
cols = c("energy","loudness","danceability","acousticness","instrumentalness","duration_ms","mode","save")
training_data = df[sample, ] %>% select(cols)
testing_data = df[-sample, ] %>% select(cols)
```

## Choice of the model

we will be testing based on Accuracy :

Bagged CART
RandomForest
Stochastic Gradient Boosting

## Bagged CART
```{r}
fitControl_bag = trainControl(method = "repeatedcv",number = 8,repeats = 3)
model_tune_bag = train(as.factor(save) ~ ., data = training_data, method = "treebag", trControl = fitControl_bag)
model_tune_bag

predict2=predict(model_tune_bag,testing_data)
Accuracy(y_pred =predict2,y_true=testing_data$save)
```



## RandomForest

```{r}
fitControl_RF = trainControl(method = "repeatedcv",number = 8,repeats = 3)
model_tune_RF = train(as.factor(save) ~ ., data = training_data, method = "rf", trControl = fitControl_RF)
model_tune_RF

predict3=predict(model_tune_RF,testing_data)
Accuracy(y_pred =predict3,y_true=testing_data$save)

```
## Stochastic Gradient Boosting

```{r}
fitControl = trainControl(method = "repeatedcv",number = 10,repeats = 3)
model_tune_boost = train(as.factor(save) ~ ., data = training_data, method = "gbm", trControl = fitControl,verbose = FALSE)
model_tune_boost

predict4=predict(model_tune_boost,testing_data)
Accuracy(y_pred =predict4,y_true=testing_data$save)
```




## Now we will use RandomForest provides the best results


base random forest
```{r}

model_Random_Forest=randomForest(save~. ,
                                 data=training_data,
                                 importance = TRUE,
                                 proximity = TRUE)
model_Random_Forest

prediction_model_Random_Forest=predict(model_Random_Forest,testing_data)
Accuracy(y_pred =prediction_model_Random_Forest,y_true=testing_data$save)

```



## We optimise randomForest

```{r}
importance(model_Random_Forest)

```

```{r}
varImpPlot(model_Random_Forest)
```

```{r}
#Create control function for training with 10 folds and keep 3 folds for training. search method is grid.
control <- trainControl(method='oob', 
                        number=6, 
                        search='grid',
                        adaptive = list(min = 5, alpha = 0.05, method = "gls", complete = TRUE))

#control2=trainControl(method = "repeatedcv",number = 10,repeats = 3)


#create tunegrid with 15 values from 1:15 for mtry to tunning model. Our train function will change number of entry variable at each split according to tunegrid. 
tunegrid <- expand.grid(.mtry = (1:5)) 
rf_gridsearch <- train(save ~ . , 
                       data = training_data,
                       method = 'rf',
                       metric = 'Accuracy',
                       tuneGrid = tunegrid,
                       trControl=control)
print(rf_gridsearch)
```

```{r}
plot(rf_gridsearch)
```

```{r}
predict5=predict(rf_gridsearch,testing_data)
Accuracy(y_pred =predict5,y_true=testing_data$save)
```

RandomForest gives us the best and more stable prediction here


# Test with full dataset training and testing (for CSV output)

```{r message = FALSE}
df <- read.csv("C:/Users/Papa/Documents/R_eval/train.csv")
cols = c("energy","loudness","danceability","acousticness","instrumentalness","duration_ms","mode","save")
df = df %>% select(cols)

cols2 = c("energy","loudness","danceability","acousticness","instrumentalness","duration_ms","mode")
df_test <- read.csv("C:/Users/Papa/Documents/R_eval/test.csv")
df_test = df_test %>% select(cols2)

training_data = df
testing_data = df_test



control <- trainControl(method='oob', 
                        number=6, 
                        search='grid',
                        adaptive = list(min = 5, alpha = 0.05, method = "gls", complete = TRUE))

tunegrid <- expand.grid(.mtry = (1:5)) 
rf_gridsearch <- train(as.factor(save) ~ ., 
                       data = training_data,
                       method = 'rf',
                       metric = 'Accuracy',
                       tuneGrid = tunegrid,
                       trControl=control)


prediction_final=predict(rf_gridsearch,testing_data)
```

```{r}
to_be_submitted = data.frame(id=rownames(df_test), save=prediction_final)
write.csv(to_be_submitted , file = "to_be_submitted.csv", row.names = F)
```

